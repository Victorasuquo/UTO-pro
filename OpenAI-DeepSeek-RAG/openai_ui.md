# UTOM: Intelligent Document Query System

This document provides detailed documentation for the "UTOM: Intelligent Document Query System" Python script. The following sections offer an in-depth look into its functionality, installation process, usage, and code walkthrough.

---

## Overview

The script is designed to build an intelligent document query system using several key technologies:

- **Streamlit** for building a user interface.
- **ChromaDB** to persist and query document embeddings.
- **OpenAI API** for generating embeddings and chat responses.
- **dotenv** for managing environment variables (e.g., API keys).

The system allows users to upload markdown files, automatically splits the text into manageable chunks, generates embeddings using OpenAI, stores them in a Chroma collection, and then enables querying of the documents. When a question is asked, the script retrieves relevant document chunks based on their embeddings and generates a structured response using a chat completion from the OpenAI API.

---

## Installation & Dependencies

### Dependencies

- Python 3.7 or higher
- [Streamlit](https://streamlit.io/)
- [python-dotenv](https://pypi.org/project/python-dotenv/)
- [ChromaDB](https://github.com/chroma-core/chroma) or similar
- [openai](https://github.com/openai/openai-python)

### Installation

1. Install the required packages using pip:

   ```
   pip install streamlit python-dotenv chromadb openai
   ```

2. Create a `.env` file in the root directory of your project and add your OpenAI API key:

   ```
   OPENAI_API_KEY=your_openai_api_key_here
   ```

3. (Optional) Set up a persistent storage directory if required by ChromaDB (default is set to "chroma_persistent_storage").

---

## Usage

1. Run the Streamlit application with the following command in your terminal:

   ```
   streamlit run <script_name.py>
   ```

2. In the web UI:
   - Upload one or more markdown (`.md`) files.
   - Wait for the system to process and store the documents.
   - Enter a question in the provided input field to query the stored documents.
   - The system will display a structured answer based on the retrieved context.

---

## Function/Class Documentation

### Function: split_text

- **Purpose**: 
  - Splits a given text into smaller chunks, facilitating easier processing and embedding for long documents.

- **Parameters**:
  - `text` (str): The input text to be split.
  - `chunk_size` (int, optional): The size of each text chunk. Defaults to 1000 characters.
  - `chunk_overlap` (int, optional): The overlap between consecutive chunks to maintain context. Defaults to 20 characters.

- **Returns**:
  - `List[str]`: A list of text chunks extracted from the input `text`.

### Function: get_openai_embedding

- **Purpose**:
  - Calls the OpenAI API to generate an embedding for a given text.

- **Parameters**:
  - `text` (str): The text for which the embedding is generated.

- **Returns**:
  - `List[float]` (or similar, depending on API response): The embedding vector for the provided text.

### Function: query_documents

- **Purpose**:
  - Queries the Chroma collection to retrieve document chunks relevant to a given question.

- **Parameters**:
  - `question` (str): The question/query for which documents are to be retrieved.
  - `n_results` (int, optional): The number of results to retrieve. Defaults to 2.

- **Returns**:
  - `List[str]`: A list of document chunks that are relevant to the input question.

### Function: generate_response

- **Purpose**:
  - Generates a response for a given question by integrating the relevant document chunks into a structured prompt and sending it to the OpenAI chat API.

- **Parameters**:
  - `question` (str): The input question.
  - `relevant_chunks` (List[str]): A list of document chunks serving as context for the answer.

- **Returns**:
  - `str`: The structured answer generated by the OpenAI chat model.

---

## Code Walkthrough

1. **Environment and Library Setup**:
   - The script imports necessary modules such as `os`, `streamlit`, `dotenv`, `chromadb`, and `openai`.
   - It loads environment variables using `load_dotenv()` to access the `OPENAI_API_KEY`.

2. **Embedding Function Initialization**:
   - An OpenAI embedding function is created with `embedding_functions.OpenAIEmbeddingFunction`, using the API key and the model `"text-embedding-3-small"`.

3. **ChromaDB Client and Collection Setup**:
   - A persistent ChromaDB client is initialized with a directory name for data persistence.
   - A collection named `document_qa_collection` is either retrieved or created for storing document embeddings.

4. **OpenAI API Client Setup**:
   - An instance of the OpenAI client is created for generating text embeddings and obtaining chat completions.

5. **Document Processing Functions**:
   - `split_text`: Handles the splitting of large documents into smaller chunks.
   - `get_openai_embedding`: Generates embeddings for the text chunks via the OpenAI API.
   - `query_documents`: Retrieves document chunks that are relevant to the user’s query.
   - `generate_response`: Builds a prompt using the retrieved context and generates a structured answer through the OpenAI chat API.

6. **Streamlit User Interface**:
   - A title is rendered using `st.title`.
   - Users can upload markdown files using `st.file_uploader`.
   - Uploaded documents are processed:
     - The full text is split into chunks.
     - For each chunk, an embedding is generated and the chunk is upserted (stored) in the Chroma collection.
   - The script then takes user input as a query, retrieves relevant document chunks, and displays the generated answer using markdown formatting.

---

## Example Output

After uploading markdown documents and entering a question, the response might look like:

```
### Answer:
Purpose:
- To provide an intelligent answer to the queried topic based on the uploaded documents.

Primary Functionality:
- Splitting documents into manageable chunks.
- Generating embeddings for each chunk.
- Retrieving and querying the stored data for relevant context.
- Generating a well-structured answer using the OpenAI chat API.
```

This output is dynamically generated by the OpenAI chat API and rendered in the Streamlit app.

---

## Error Handling

- **Environment Variables**: 
  - The script assumes that the `.env` file will contain the `OPENAI_API_KEY`. If it is missing, the OpenAI API calls may fail.
  
- **File Uploads**:
  - The code checks if files are uploaded before processing them. If no files are uploaded, it will simply not execute the document processing steps.

- **API Responses**:
  - The script directly uses responses from OpenAI's API. It assumes valid responses are returned and does not add explicit retry or error logging mechanisms.

- **Chroma Database**:
  - The script uses `get_or_create_collection` which gracefully handles the retrieval or initialization of the document collection.
  
While the script does not include extensive try/except blocks, it relies on the robustness of the imported libraries to manage errors and exceptions at runtime.

---

This documentation serves as a comprehensive guide for understanding, installing, and using the "UTOM: Intelligent Document Query System" script.