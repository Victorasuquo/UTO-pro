{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 Markdown files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Load data from .md files in the \"files\" folder\n",
    "def load_md_files(input_dir):\n",
    "    docs = []\n",
    "    for root, _, files in os.walk(input_dir):  # Walk through directory\n",
    "        for file in files:\n",
    "            if file.endswith(\".md\"):  # Only load .md files\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    docs.append({\n",
    "                        \"filename\": file,\n",
    "                        \"content\": f.read()\n",
    "                    })\n",
    "    return docs\n",
    "\n",
    "# Specify the input directory\n",
    "input_dir = './files'\n",
    "docs = load_md_files(input_dir)\n",
    "\n",
    "# Verify loaded files\n",
    "print(f\"Loaded {len(docs)} Markdown files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'filename': 'Readme.md', 'content': '# Project Workflow Documentation\\n\\nThis documentation provides a detailed overview of the code and instructions on how to use it. The code is a Flask application that integrates with MongoDB and OpenAI\\'s GPT model to manage a project workflow.\\n\\n![API calls](image.png)\\n*Figure 1: API calls*\\n## Table of Contents\\n\\n1. [Introduction](#introduction)\\n2. [Prerequisites](#prerequisites)\\n3. [Installation](#installation)\\n4. [Configuration](#configuration)\\n5. [Usage](#usage)\\n6. [Endpoints](#endpoints)\\n7. [Code Explanation](#code-explanation)\\n8. [Contributing](#contributing)\\n9. [License](#license)\\n\\n## Introduction\\n\\nThis Flask application is designed to manage a project workflow by interacting with a MongoDB database and OpenAI\\'s GPT model. It provides endpoints to initialize a project, ask questions, generate user stories, and generate development plans.\\n\\n## Prerequisites\\n\\nBefore you begin, ensure you have met the following requirements:\\n\\n- Python 3.x installed\\n- MongoDB instance running\\n- OpenAI API key\\n\\n## Installation\\n\\n1. Clone the repository:\\n\\n```bash\\ngit clone <repository_url>\\ncd <repository_directory>\\n```\\n\\n2. Install the required dependencies:\\n\\n```bash\\npip install -r requirements.txt\\n```\\n\\n## Configuration\\n\\n1. Create a `.env` file in the root directory of the project.\\n2. Add the following environment variables to the `.env` file:\\n\\n```env\\nOPEN_AI=your_openai_api_key\\nMONGO_URI=your_mongodb_uri\\n```\\n\\n## Usage\\n\\n1. Start the Flask application:\\n\\n```bash\\npython app.py\\n```\\n\\n2. The application will be running on `http://127.0.0.1:5000/`.\\n\\n## Endpoints\\n\\n### Initialize Project\\n\\n- **URL:** `/initialize`\\n- **Method:** `POST`\\n- **Description:** Initializes a new project state in the database.\\n- **Response:**\\n\\n```json\\n{\\n  \"state_id\": \"project_id\"\\n}\\n```\\n\\n### Ask Question\\n\\n- **URL:** `/ask_question`\\n- **Method:** `POST`\\n- **Description:** Asks a question and updates the project context.\\n- **Request Body:**\\n\\n```json\\n{\\n  \"state_id\": \"project_id\",\\n  \"custom_question\": \"optional_custom_question\",\\n  \"answer\": \"user_answer\"\\n}\\n```\\n\\n- **Response:**\\n\\n```json\\n{\\n  \"state_id\": \"project_id\",\\n  \"question\": \"generated_question\"\\n}\\n```\\n\\n### Generate User Stories\\n\\n- **URL:** `/generate_stories`\\n- **Method:** `POST`\\n- **Description:** Generates user stories based on the project context.\\n- **Request Body:**\\n\\n```json\\n{\\n  \"state_id\": \"project_id\"\\n}\\n```\\n\\n- **Response:**\\n\\n```json\\n{\\n  \"state_id\": \"project_id\",\\n  \"stories\": \"generated_stories\"\\n}\\n```\\n\\n### Generate Development Plans\\n\\n- **URL:** `/generate_development_plans`\\n- **Method:** `POST`\\n- **Description:** Generates development plans based on the project context.\\n- **Request Body:**\\n\\n```json\\n{\\n  \"state_id\": \"project_id\"\\n}\\n```\\n\\n- **Response:**\\n\\n```json\\n{\\n  \"state_id\": \"project_id\",\\n  \"agent_responses\": \"generated_responses\",\\n  \"tasks\": \"generated_tasks\"\\n}\\n```\\n\\n## Code Explanation\\n\\n### Imports\\n\\n```python\\nimport os\\nfrom pymongo import MongoClient\\nfrom bson import ObjectId\\nfrom dotenv import load_dotenv\\nimport openai\\nfrom flask import Flask, request, jsonify\\n```\\n\\n### Load Environment Variables\\n\\n```python\\nload_dotenv()\\nopenai.api_key = os.getenv(\"OPEN_AI\")\\nMONGO_URI = os.getenv(\"MONGO_URI\")\\nDATABASE_NAME = \"project_workflow\"\\n```\\n\\n### Initialize Flask App\\n\\n```python\\napp = Flask(__name__)\\n```\\n\\n### Initialize MongoDB Client\\n\\n```python\\ndef get_db():\\n    client = MongoClient(MONGO_URI)\\n    return client[DATABASE_NAME]\\n```\\n\\n### Utility Functions\\n\\n#### Save Data to the Database\\n\\n```python\\ndef save_to_db(collection_name, data):\\n    db = get_db()\\n    collection = db[collection_name]\\n    result = collection.insert_one(data)\\n    return str(result.inserted_id)\\n```\\n\\n#### Retrieve Data by ID\\n\\n```python\\ndef get_from_db(collection_name, doc_id):\\n    db = get_db()\\n    collection = db[collection_name]\\n    return collection.find_one({\"_id\": ObjectId(doc_id)})\\n```\\n\\n### Initialize Project State\\n\\n```python\\n@app.route(\\'/initialize\\', methods=[\\'POST\\'])\\ndef initialize_project():\\n    project_state = {\\n        \"context\": \"\",\\n        \"questions_asked\": 0,\\n        \"stories\": None,\\n        \"parsed_stories\": None,\\n        \"selected_story\": None,\\n        \"user_flow\": [],\\n        \"agent_responses\": {},\\n        \"tasks\": {}\\n    }\\n    project_id = save_to_db(\"project_states\", project_state)\\n    return jsonify({\"state_id\": project_id}), 201\\n```\\n\\n### Update Project State\\n\\n```python\\ndef update_project_state(state_id, updates):\\n    db = get_db()\\n    collection = db[\"project_states\"]\\n    collection.update_one({\"_id\": ObjectId(state_id)}, {\"$set\": updates})\\n```\\n\\n### Query OpenAI GPT Model\\n\\n```python\\ndef query_gpt(prompt, model=\"gpt-4o-mini\", max_tokens=500, temperature=0.7):\\n\\n    \"\"\"\\n    Query the OpenAI GPT model with a given prompt.\\n    Args:\\n        prompt (str): The input prompt for the GPT model.\\n    Returns:\\n        str: The generated response from the GPT model.\\n    \"\"\"\\n    response = client.chat.completions.create(\\n        model=model,  # or another appropriate model\\n        messages=[\\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\\n            {\"role\": \"user\", \"content\": prompt}\\n        ],\\n        max_tokens=max_tokens,\\n        temperature=temperature\\n    )\\n    return response.choices[0].message.content\\n```\\n\\n### Ask a Question and Update Context\\n\\n```python\\n@app.route(\\'/ask_question\\', methods=[\\'POST\\'])\\ndef ask_question():\\n    data = request.json\\n    state_id = data.get(\"state_id\")\\n    custom_question = data.get(\"custom_question\")\\n    state = get_from_db(\"project_states\", state_id)\\n\\n    if state[\"questions_asked\"] == 0:\\n        question = \"What kind of application do you want to build? Please provide a detailed description.\"\\n    elif custom_question:\\n        question = custom_question\\n    else:\\n        prompt = f\"Based on this context, ask a clarifying question:\\\\n\\\\n{state[\\'context\\']}\"\\n        question = query_gpt(prompt)\\n\\n    answer = data.get(\"answer\")\\n    context_update = f\"Q{state[\\'questions_asked\\'] + 1}: {question}\\\\nA{state[\\'questions_asked\\'] + 1}: {answer}\\\\n\"\\n\\n    state[\"context\"] += context_update\\n    state[\"questions_asked\"] += 1\\n    state[\"user_flow\"].append({\"question\": question, \"answer\": answer})\\n\\n    update_project_state(state_id, state)\\n    return jsonify({\"state_id\": state_id, \"question\": question}), 200\\n```\\n\\n### Generate User Stories\\n\\n```python\\n@app.route(\\'/generate_stories\\', methods=[\\'POST\\'])\\ndef generate_stories():\\n    data = request.json\\n    state_id = data.get(\"state_id\")\\n    state = get_from_db(\"project_states\", state_id)\\n    prompt = f\"\"\"Based on this context, generate 5 distinct user stories.\\n    Format each story exactly like this:\\n\\n    [STORY_TITLE]: <creative theme or say or expression based on the context of the user story>\\n    [USER_TYPE]: <type of user this story is about>\\n    [USER_NEED]: <3-4 sentences what the user wants to accomplish>\\n    [ACCEPTANCE_CRITERIA]: <key requirements for the story to be complete>\\n    [VALUE]: <business value or user benefit>\\n\\n    Context:\\n    {state[\\'context\\']}\"\"\"\\n\\n    stories = query_gpt(prompt, max_tokens=800)\\n    parsed_stories = parse_stories(stories)\\n\\n    state[\"stories\"] = stories\\n    state[\"parsed_stories\"] = parsed_stories\\n    update_project_state(state_id, state)\\n    return jsonify({\"state_id\": state_id, \"stories\": parsed_stories}), 200\\n```\\n\\n### Parse Stories into Structured Format\\n\\n```python\\ndef parse_stories(stories_text):\\n    stories = []\\n    current_story = {}\\n\\n    for line in stories_text.split(\\'\\\\n\\'):\\n        line = line.strip()\\n        if not line:\\n            if current_story:\\n                stories.append(current_story)\\n                current_story = {}\\n            continue\\n\\n        for tag in [\\'STORY_TITLE\\', \\'USER_TYPE\\', \\'USER_NEED\\', \\'ACCEPTANCE_CRITERIA\\', \\'VALUE\\']:\\n            if line.startswith(f\\'[{tag}]:\\'):\\n                current_story[tag] = line.split(\\':\\', 1)[1].strip()\\n                break\\n\\n    if current_story:\\n        stories.append(current_story)\\n\\n    return stories\\n```\\n\\n### Generate Development Plans\\n\\n```python\\n@app.route(\\'/generate_development_plans\\', methods=[\\'POST\\'])\\ndef generate_development_plans():\\n    data = request.json\\n    state_id = data.get(\"state_id\")\\n    state = get_from_db(\"project_states\", state_id)\\n    agent_types = [\\'frontend\\', \\'backend\\', \\'design\\', \\'product\\', \\'ai\\']\\n    responses = {}\\n    tasks = {}\\n\\n    for agent_type in agent_types:\\n        prompt = f\"As a {agent_type} developer, what are the key components needed for the project?\\\\n\\\\nContext:\\\\n{state[\\'context\\']}\"\\n        response = query_gpt(prompt)\\n        responses[agent_type] = response\\n        tasks[agent_type] = extract_tasks(response)\\n\\n    state[\"agent_responses\"] = responses\\n    state[\"tasks\"] = tasks\\n    update_project_state(state_id, state)\\n    return jsonify({\"state_id\": state_id, \"agent_responses\": responses, \"tasks\": tasks}), 200\\n```\\n\\n### Extract Tasks from a Response\\n\\n```python\\ndef extract_tasks(response):\\n    tasks = []\\n    for line in response.split(\\'\\\\n\\'):\\n        line = line.strip()\\n        if line.startswith((\\'-\\', \\'*\\', \\'1.\\', \\'2.\\', \\'3.\\')) or \\':\\' in line:\\n            tasks.append(line.lstrip(\\'-*123456789. \\').strip())\\n    return tasks\\n```\\n\\n### Run the Application\\n\\n```python\\nif __name__ == \"__main__\":\\n    app.run(debug=True)\\n```\\n\\n## Contributing\\n\\nContributions are welcome! Please follow these steps:\\n\\n1. Fork the repository.\\n2. Create a new branch (`git checkout -b feature-branch`).\\n3. Commit your changes (`git commit -am \\'Add new feature\\'`).\\n4. Push to the branch (`git push origin feature-branch`).\\n5. Create a new Pull Request.\\n\\n## License\\n\\nThis project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.'}]\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "print(docs[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating embeddings: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Generated embeddings for 0 documents.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document, Settings\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # Automatically uses the environment variable\n",
    ")\n",
    "\n",
    "# Define a reusable embedding function using the new OpenAI client\n",
    "class OpenAIEmbedModel:\n",
    "    def __init__(self, client, model_name=\"text-embedding-ada-002\"):\n",
    "        self.client = client\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def embed(self, texts):\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]  # Wrap single string in a list\n",
    "        try:\n",
    "            response = self.client.embeddings.create(\n",
    "                model=self.model_name,\n",
    "                input=texts\n",
    "            )\n",
    "            # Extract embeddings for each text\n",
    "            embeddings = [data['embedding'] for data in response['data']]\n",
    "            return np.array(embeddings)  # Convert to numpy array\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating embeddings: {e}\")\n",
    "            return None\n",
    "\n",
    "# Initialize the embedding model with the new client\n",
    "embed_model = OpenAIEmbedModel(client=client)\n",
    "\n",
    "# Load documents from Markdown files\n",
    "docs = SimpleDirectoryReader(\"./files\", required_exts=[\".md\"]).load_data()\n",
    "\n",
    "# Create a list to store document embeddings\n",
    "doc_embeddings = []\n",
    "\n",
    "# Loop through each Markdown document\n",
    "for doc in docs:\n",
    "    doc_text = doc.text  # Use the .text attribute to access the content\n",
    "\n",
    "    # Generate embeddings for the document text\n",
    "    embedding = embed_model.embed(doc_text)\n",
    "\n",
    "    if embedding is not None:\n",
    "        # Store the embedding for this document\n",
    "        doc_embeddings.append({\n",
    "            \"filename\": doc.filename,  # Access filename attribute\n",
    "            \"embedding\": embedding\n",
    "        })\n",
    "\n",
    "# Confirm embeddings were generated\n",
    "print(f\"Generated embeddings for {len(doc_embeddings)} documents.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now `doc_embeddings` contains the embeddings for each document\n",
    "# print(doc_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'VectorStoreIndex' has no attribute 'from_embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VectorStoreIndex\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Create the VectorStoreIndex with the document embeddings\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mVectorStoreIndex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_embeddings\u001b[49m(doc_embeddings)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# The index is now ready for querying\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex created and ready for querying.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'VectorStoreIndex' has no attribute 'from_embeddings'"
     ]
    }
   ],
   "source": [
    "# Now, use the generated embeddings to create the vector store index\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Create the VectorStoreIndex with the document embeddings\n",
    "index = VectorStoreIndex.from_embeddings(doc_embeddings)\n",
    "\n",
    "# The index is now ready for querying\n",
    "print(\"Index created and ready for querying.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Engine\n",
    "from llama_index.llms.openai import OpenAI as LLM_OpenAI\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# Setting up the OpenAI LLM (using OpenAI's API)\n",
    "llm = LLM_OpenAI(model=\"gpt-4o-mini\", request_timeout=300.0)\n",
    "\n",
    "# Specify the LLM to be used in the settings\n",
    "Settings.llm = llm\n",
    "\n",
    "# Setup a query engine on the index previously created (assumes `index` is already defined)\n",
    "query_engine = index.as_query_engine(streaming=True, similarity_top_k=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI as LLM_OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "# Set up OpenAI as the LLM (using OpenAI's API)\n",
    "llm = LLM_OpenAI(model=\"gpt-4o-mini\", request_timeout=300.0)\n",
    "\n",
    "# Set up the OpenAI embedding model\n",
    "embed_model = OpenAIEmbedding(model_name=\"text-embedding-ada-002\")\n",
    "\n",
    "# Load documents\n",
    "documents = SimpleDirectoryReader(\"./files\").load_data()\n",
    "\n",
    "# Create the index with the OpenAI embedding model\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "\n",
    "# Create the query engine with OpenAI as the LLM\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "\n",
    "# Query the engine\n",
    "response = query_engine.query(\"Give me the detailed aim of the project as seen from the markdown.\")\n",
    "\n",
    "# Print the response\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
