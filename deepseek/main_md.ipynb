{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads data from all PDF files in the \"files\" folder within the current directory.\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# load data\n",
    "loader = SimpleDirectoryReader(\n",
    "            input_dir = './files',  # Points to the \"files\" folder in the current directory\n",
    "            required_exts=[\".md\"],\n",
    "            recursive=True\n",
    "        )\n",
    "docs = loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id_='40fdb7df-ece6-4963-9cd4-b41bb9f43ef0', embedding=None, metadata={'file_path': 'c:\\\\Users\\\\Administrator\\\\Desktop\\\\deepseek_rag\\\\files\\\\Readme.md', 'file_name': 'Readme.md', 'file_size': 9971, 'creation_date': '2025-01-27', 'last_modified_date': '2025-01-27'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# Project Workflow Documentation\\r\\n\\r\\nThis documentation provides a detailed overview of the code and instructions on how to use it. The code is a Flask application that integrates with MongoDB and OpenAI\\'s GPT model to manage a project workflow.\\r\\n\\r\\n![API calls](image.png)\\r\\n*Figure 1: API calls*\\r\\n## Table of Contents\\r\\n\\r\\n1. [Introduction](#introduction)\\r\\n2. [Prerequisites](#prerequisites)\\r\\n3. [Installation](#installation)\\r\\n4. [Configuration](#configuration)\\r\\n5. [Usage](#usage)\\r\\n6. [Endpoints](#endpoints)\\r\\n7. [Code Explanation](#code-explanation)\\r\\n8. [Contributing](#contributing)\\r\\n9. [License](#license)\\r\\n\\r\\n## Introduction\\r\\n\\r\\nThis Flask application is designed to manage a project workflow by interacting with a MongoDB database and OpenAI\\'s GPT model. It provides endpoints to initialize a project, ask questions, generate user stories, and generate development plans.\\r\\n\\r\\n## Prerequisites\\r\\n\\r\\nBefore you begin, ensure you have met the following requirements:\\r\\n\\r\\n- Python 3.x installed\\r\\n- MongoDB instance running\\r\\n- OpenAI API key\\r\\n\\r\\n## Installation\\r\\n\\r\\n1. Clone the repository:\\r\\n\\r\\n```bash\\r\\ngit clone <repository_url>\\r\\ncd <repository_directory>\\r\\n```\\r\\n\\r\\n2. Install the required dependencies:\\r\\n\\r\\n```bash\\r\\npip install -r requirements.txt\\r\\n```\\r\\n\\r\\n## Configuration\\r\\n\\r\\n1. Create a `.env` file in the root directory of the project.\\r\\n2. Add the following environment variables to the `.env` file:\\r\\n\\r\\n```env\\r\\nOPEN_AI=your_openai_api_key\\r\\nMONGO_URI=your_mongodb_uri\\r\\n```\\r\\n\\r\\n## Usage\\r\\n\\r\\n1. Start the Flask application:\\r\\n\\r\\n```bash\\r\\npython app.py\\r\\n```\\r\\n\\r\\n2. The application will be running on `http://127.0.0.1:5000/`.\\r\\n\\r\\n## Endpoints\\r\\n\\r\\n### Initialize Project\\r\\n\\r\\n- **URL:** `/initialize`\\r\\n- **Method:** `POST`\\r\\n- **Description:** Initializes a new project state in the database.\\r\\n- **Response:**\\r\\n\\r\\n```json\\r\\n{\\r\\n  \"state_id\": \"project_id\"\\r\\n}\\r\\n```\\r\\n\\r\\n### Ask Question\\r\\n\\r\\n- **URL:** `/ask_question`\\r\\n- **Method:** `POST`\\r\\n- **Description:** Asks a question and updates the project context.\\r\\n- **Request Body:**\\r\\n\\r\\n```json\\r\\n{\\r\\n  \"state_id\": \"project_id\",\\r\\n  \"custom_question\": \"optional_custom_question\",\\r\\n  \"answer\": \"user_answer\"\\r\\n}\\r\\n```\\r\\n\\r\\n- **Response:**\\r\\n\\r\\n```json\\r\\n{\\r\\n  \"state_id\": \"project_id\",\\r\\n  \"question\": \"generated_question\"\\r\\n}\\r\\n```\\r\\n\\r\\n### Generate User Stories\\r\\n\\r\\n- **URL:** `/generate_stories`\\r\\n- **Method:** `POST`\\r\\n- **Description:** Generates user stories based on the project context.\\r\\n- **Request Body:**\\r\\n\\r\\n```json\\r\\n{\\r\\n  \"state_id\": \"project_id\"\\r\\n}\\r\\n```\\r\\n\\r\\n- **Response:**\\r\\n\\r\\n```json\\r\\n{\\r\\n  \"state_id\": \"project_id\",\\r\\n  \"stories\": \"generated_stories\"\\r\\n}\\r\\n```\\r\\n\\r\\n### Generate Development Plans\\r\\n\\r\\n- **URL:** `/generate_development_plans`\\r\\n- **Method:** `POST`\\r\\n- **Description:** Generates development plans based on the project context.\\r\\n- **Request Body:**\\r\\n\\r\\n```json\\r\\n{\\r\\n  \"state_id\": \"project_id\"\\r\\n}\\r\\n```\\r\\n\\r\\n- **Response:**\\r\\n\\r\\n```json\\r\\n{\\r\\n  \"state_id\": \"project_id\",\\r\\n  \"agent_responses\": \"generated_responses\",\\r\\n  \"tasks\": \"generated_tasks\"\\r\\n}\\r\\n```\\r\\n\\r\\n## Code Explanation\\r\\n\\r\\n### Imports\\r\\n\\r\\n```python\\r\\nimport os\\r\\nfrom pymongo import MongoClient\\r\\nfrom bson import ObjectId\\r\\nfrom dotenv import load_dotenv\\r\\nimport openai\\r\\nfrom flask import Flask, request, jsonify\\r\\n```\\r\\n\\r\\n### Load Environment Variables\\r\\n\\r\\n```python\\r\\nload_dotenv()\\r\\nopenai.api_key = os.getenv(\"OPEN_AI\")\\r\\nMONGO_URI = os.getenv(\"MONGO_URI\")\\r\\nDATABASE_NAME = \"project_workflow\"\\r\\n```\\r\\n\\r\\n### Initialize Flask App\\r\\n\\r\\n```python\\r\\napp = Flask(__name__)\\r\\n```\\r\\n\\r\\n### Initialize MongoDB Client\\r\\n\\r\\n```python\\r\\ndef get_db():\\r\\n    client = MongoClient(MONGO_URI)\\r\\n    return client[DATABASE_NAME]\\r\\n```\\r\\n\\r\\n### Utility Functions\\r\\n\\r\\n#### Save Data to the Database\\r\\n\\r\\n```python\\r\\ndef save_to_db(collection_name, data):\\r\\n    db = get_db()\\r\\n    collection = db[collection_name]\\r\\n    result = collection.insert_one(data)\\r\\n    return str(result.inserted_id)\\r\\n```\\r\\n\\r\\n#### Retrieve Data by ID\\r\\n\\r\\n```python\\r\\ndef get_from_db(collection_name, doc_id):\\r\\n    db = get_db()\\r\\n    collection = db[collection_name]\\r\\n    return collection.find_one({\"_id\": ObjectId(doc_id)})\\r\\n```\\r\\n\\r\\n### Initialize Project State\\r\\n\\r\\n```python\\r\\n@app.route(\\'/initialize\\', methods=[\\'POST\\'])\\r\\ndef initialize_project():\\r\\n    project_state = {\\r\\n        \"context\": \"\",\\r\\n        \"questions_asked\": 0,\\r\\n        \"stories\": None,\\r\\n        \"parsed_stories\": None,\\r\\n        \"selected_story\": None,\\r\\n        \"user_flow\": [],\\r\\n        \"agent_responses\": {},\\r\\n        \"tasks\": {}\\r\\n    }\\r\\n    project_id = save_to_db(\"project_states\", project_state)\\r\\n    return jsonify({\"state_id\": project_id}), 201\\r\\n```\\r\\n\\r\\n### Update Project State\\r\\n\\r\\n```python\\r\\ndef update_project_state(state_id, updates):\\r\\n    db = get_db()\\r\\n    collection = db[\"project_states\"]\\r\\n    collection.update_one({\"_id\": ObjectId(state_id)}, {\"$set\": updates})\\r\\n```\\r\\n\\r\\n### Query OpenAI GPT Model\\r\\n\\r\\n```python\\r\\ndef query_gpt(prompt, model=\"gpt-4o-mini\", max_tokens=500, temperature=0.7):\\r\\n\\r\\n    \"\"\"\\r\\n    Query the OpenAI GPT model with a given prompt.\\r\\n    Args:\\r\\n        prompt (str): The input prompt for the GPT model.\\r\\n    Returns:\\r\\n        str: The generated response from the GPT model.\\r\\n    \"\"\"\\r\\n    response = client.chat.completions.create(\\r\\n        model=model,  # or another appropriate model\\r\\n        messages=[\\r\\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\\r\\n            {\"role\": \"user\", \"content\": prompt}\\r\\n        ],\\r\\n        max_tokens=max_tokens,\\r\\n        temperature=temperature\\r\\n    )\\r\\n    return response.choices[0].message.content\\r\\n```\\r\\n\\r\\n### Ask a Question and Update Context\\r\\n\\r\\n```python\\r\\n@app.route(\\'/ask_question\\', methods=[\\'POST\\'])\\r\\ndef ask_question():\\r\\n    data = request.json\\r\\n    state_id = data.get(\"state_id\")\\r\\n    custom_question = data.get(\"custom_question\")\\r\\n    state = get_from_db(\"project_states\", state_id)\\r\\n\\r\\n    if state[\"questions_asked\"] == 0:\\r\\n        question = \"What kind of application do you want to build? Please provide a detailed description.\"\\r\\n    elif custom_question:\\r\\n        question = custom_question\\r\\n    else:\\r\\n        prompt = f\"Based on this context, ask a clarifying question:\\\\n\\\\n{state[\\'context\\']}\"\\r\\n        question = query_gpt(prompt)\\r\\n\\r\\n    answer = data.get(\"answer\")\\r\\n    context_update = f\"Q{state[\\'questions_asked\\'] + 1}: {question}\\\\nA{state[\\'questions_asked\\'] + 1}: {answer}\\\\n\"\\r\\n\\r\\n    state[\"context\"] += context_update\\r\\n    state[\"questions_asked\"] += 1\\r\\n    state[\"user_flow\"].append({\"question\": question, \"answer\": answer})\\r\\n\\r\\n    update_project_state(state_id, state)\\r\\n    return jsonify({\"state_id\": state_id, \"question\": question}), 200\\r\\n```\\r\\n\\r\\n### Generate User Stories\\r\\n\\r\\n```python\\r\\n@app.route(\\'/generate_stories\\', methods=[\\'POST\\'])\\r\\ndef generate_stories():\\r\\n    data = request.json\\r\\n    state_id = data.get(\"state_id\")\\r\\n    state = get_from_db(\"project_states\", state_id)\\r\\n    prompt = f\"\"\"Based on this context, generate 5 distinct user stories.\\r\\n    Format each story exactly like this:\\r\\n\\r\\n    [STORY_TITLE]: <creative theme or say or expression based on the context of the user story>\\r\\n    [USER_TYPE]: <type of user this story is about>\\r\\n    [USER_NEED]: <3-4 sentences what the user wants to accomplish>\\r\\n    [ACCEPTANCE_CRITERIA]: <key requirements for the story to be complete>\\r\\n    [VALUE]: <business value or user benefit>\\r\\n\\r\\n    Context:\\r\\n    {state[\\'context\\']}\"\"\"\\r\\n\\r\\n    stories = query_gpt(prompt, max_tokens=800)\\r\\n    parsed_stories = parse_stories(stories)\\r\\n\\r\\n    state[\"stories\"] = stories\\r\\n    state[\"parsed_stories\"] = parsed_stories\\r\\n    update_project_state(state_id, state)\\r\\n    return jsonify({\"state_id\": state_id, \"stories\": parsed_stories}), 200\\r\\n```\\r\\n\\r\\n### Parse Stories into Structured Format\\r\\n\\r\\n```python\\r\\ndef parse_stories(stories_text):\\r\\n    stories = []\\r\\n    current_story = {}\\r\\n\\r\\n    for line in stories_text.split(\\'\\\\n\\'):\\r\\n        line = line.strip()\\r\\n        if not line:\\r\\n            if current_story:\\r\\n                stories.append(current_story)\\r\\n                current_story = {}\\r\\n            continue\\r\\n\\r\\n        for tag in [\\'STORY_TITLE\\', \\'USER_TYPE\\', \\'USER_NEED\\', \\'ACCEPTANCE_CRITERIA\\', \\'VALUE\\']:\\r\\n            if line.startswith(f\\'[{tag}]:\\'):\\r\\n                current_story[tag] = line.split(\\':\\', 1)[1].strip()\\r\\n                break\\r\\n\\r\\n    if current_story:\\r\\n        stories.append(current_story)\\r\\n\\r\\n    return stories\\r\\n```\\r\\n\\r\\n### Generate Development Plans\\r\\n\\r\\n```python\\r\\n@app.route(\\'/generate_development_plans\\', methods=[\\'POST\\'])\\r\\ndef generate_development_plans():\\r\\n    data = request.json\\r\\n    state_id = data.get(\"state_id\")\\r\\n    state = get_from_db(\"project_states\", state_id)\\r\\n    agent_types = [\\'frontend\\', \\'backend\\', \\'design\\', \\'product\\', \\'ai\\']\\r\\n    responses = {}\\r\\n    tasks = {}\\r\\n\\r\\n    for agent_type in agent_types:\\r\\n        prompt = f\"As a {agent_type} developer, what are the key components needed for the project?\\\\n\\\\nContext:\\\\n{state[\\'context\\']}\"\\r\\n        response = query_gpt(prompt)\\r\\n        responses[agent_type] = response\\r\\n        tasks[agent_type] = extract_tasks(response)\\r\\n\\r\\n    state[\"agent_responses\"] = responses\\r\\n    state[\"tasks\"] = tasks\\r\\n    update_project_state(state_id, state)\\r\\n    return jsonify({\"state_id\": state_id, \"agent_responses\": responses, \"tasks\": tasks}), 200\\r\\n```\\r\\n\\r\\n### Extract Tasks from a Response\\r\\n\\r\\n```python\\r\\ndef extract_tasks(response):\\r\\n    tasks = []\\r\\n    for line in response.split(\\'\\\\n\\'):\\r\\n        line = line.strip()\\r\\n        if line.startswith((\\'-\\', \\'*\\', \\'1.\\', \\'2.\\', \\'3.\\')) or \\':\\' in line:\\r\\n            tasks.append(line.lstrip(\\'-*123456789. \\').strip())\\r\\n    return tasks\\r\\n```\\r\\n\\r\\n### Run the Application\\r\\n\\r\\n```python\\r\\nif __name__ == \"__main__\":\\r\\n    app.run(debug=True)\\r\\n```\\r\\n\\r\\n## Contributing\\r\\n\\r\\nContributions are welcome! Please follow these steps:\\r\\n\\r\\n1. Fork the repository.\\r\\n2. Create a new branch (`git checkout -b feature-branch`).\\r\\n3. Commit your changes (`git commit -am \\'Add new feature\\'`).\\r\\n4. Push to the branch (`git push origin feature-branch`).\\r\\n5. Create a new Pull Request.\\r\\n\\r\\n## License\\r\\n\\r\\nThis project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]\n"
     ]
    }
   ],
   "source": [
    "print(docs[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer from Hugging Face\n",
    "model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define a reusable embedding function for documents\n",
    "class CustomEmbedModel:\n",
    "    def __init__(self, model_name=\"BAAI/bge-large-en-v1.5\"):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "\n",
    "    def embed(self, texts):\n",
    "        # If a single string is provided, wrap it in a list\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        # Tokenize the input text\n",
    "        inputs = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {key: value.to(self.device) for key, value in inputs.items()}\n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1)  # Mean pooling\n",
    "        return embeddings.cpu().numpy()  # Convert to numpy array for compatibility\n",
    "\n",
    "# Initialize the embedding model\n",
    "embed_model = CustomEmbedModel()\n",
    "\n",
    "# Process each document\n",
    "doc_embeddings = []\n",
    "\n",
    "# Loop through each document\n",
    "for doc in docs:\n",
    "    # Extract text from the document (this will depend on the format of the doc object from SimpleDirectoryReader)\n",
    "    doc_text = doc.text  # Assuming doc.text contains the extracted text\n",
    "\n",
    "    # Generate embeddings for the document text\n",
    "    embedding = embed_model.embed(doc_text)\n",
    "\n",
    "    # Store the embedding for this document\n",
    "    doc_embeddings.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.22960638,  0.07712092, -0.55491114, ..., -0.27139044,\n",
      "         0.14393172,  0.7111468 ]], shape=(1, 1024), dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# Now `doc_embeddings` contains the embeddings for each document\n",
    "print(doc_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector databases\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document, Settings\n",
    "\n",
    "# Set the HuggingFace embedding model in settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# Set the HuggingFace model (BAAI/bge-small-en-v1.5)\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# Load documents from Markdown files\n",
    "documents = SimpleDirectoryReader(\"./files\", required_exts=[\".md\"]).load_data()\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# The index is now ready for querying\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Engine\n",
    "\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# Setting up the LLM (Ollama) with the deepseek-r1:1.5b model\n",
    "\n",
    "llm = Ollama(model=\"deepseek-r1:1.5b\", request_timeout=300.0)\n",
    "\n",
    "# Specify the LLM to be used in the settings\n",
    "\n",
    "Settings.llm = llm\n",
    "\n",
    "# Setup a query engine on the index previously created (assumes `index` is already defined)\n",
    "\n",
    "query_engine = index.as_query_engine(streaming=True, similarity_top_k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out the detailed aims of the project based on the provided context. The user has given a file path called Readme.md, which seems to be a documentation file for their Flask application. Looking at the introduction section, it mentions that this is a project workflow management system using MongoDB and OpenAI's GPT model.\n",
      "\n",
      "The endpoints in the documentation include several parts like Initialize, Ask Question, Generate User Stories, and Generate Development Plans. The user also provided some example code snippets, but I'm focusing on understanding the aims from the context information, which includes the Readme.md file.\n",
      "\n",
      "In the Readme.md file, the Introduction section talks about this being a project workflow management system. It mentions interacting with a MongoDB database and using OpenAI's GPT model for various tasks. The example code provided shows functions under different routes like /initialize, /ask_question, etc., but those are probably placeholders or examples, not actual parts of their application.\n",
      "\n",
      "Since the focus is on the aims from the context information, which includes the Readme.md file, I should extract what was discussed in that file. The Introduction states it's designed to manage a project workflow using MongoDB and GPT. So, the main aim is probably about managing workflows effectively through these technologies.\n",
      "\n",
      "I need to make sure not to add any external knowledge beyond the provided context. Also, since the user asked for detailed aims based on the markdown, I should stick strictly to that document without adding prior knowledge or unrelated points.\n",
      "</think>\n",
      "\n",
      "The detailed aims of the project as outlined in the Readme.md file are:\n",
      "\n",
      "1. **Effective Project Workflow Management**: The system is designed to streamline and manage workflows across projects using MongoDB for database integration and OpenAI's GPT model for generating user stories and development plans.\n",
      "\n",
      "2. **Integration with MongoDB Database**: The application leverages a MongoDB instance, allowing for structured storage of project data and facilitating complex database operations necessary for managing workflows.\n",
      "\n",
      "3. **OpenAI's GPT Integration**: The system incorporates the GPT model to generate user stories and development plans dynamically based on project context, enhancing creativity and efficiency in workflow planning.\n",
      "\n",
      "4. **User-Centric Workflow Management**: By interacting with a MongoDB database and leveraging GPT, the application provides a user-centric approach to managing workflows, ensuring that all necessary steps are completed smoothly.\n",
      "\n",
      "These aims highlight how the system integrates advanced technologies to optimize project workflows.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "# Set up DeepSeek as the LLM\n",
    "llm = Ollama(model=\"deepseek-r1:1.5b\", request_timeout=300.0)\n",
    "\n",
    "# Set up the local embedding model\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# Load documents\n",
    "documents = SimpleDirectoryReader(\"./files\").load_data()\n",
    "\n",
    "# Create the index with the embedding model\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "\n",
    "# Create the query engine with DeepSeek as the LLM\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "\n",
    "# Query the engine\n",
    "response = query_engine.query(\"Give me the detailed aim of the project has seen from the markdown.\")\n",
    "\n",
    "# Print the response\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
